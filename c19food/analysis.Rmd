---
title: "Covid-19 Food Systems Database Analysis"
author: "Robert Berry"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: yes
    toc_float: yes
    toc_depth: 4
    smooth_scroll: yes
    theme: united
    code_folding: hide
    highlight: tango
  pdf_document:
    toc: yes
    toc_depth: '4'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE)
```

The aim of this website is to report the ongoing analysis of the CCRI's [COVID-19 and Food Systems Database](http://www.ccri.ac.uk/covid19-food-db/). The current analysis is based on a version of the database downloaded on **14/01/2021**.

All of the tables and plots shown on this website can be accessed as individual files [HERE](https://connectglosac-my.sharepoint.com/:f:/g/personal/s2111547_glos_ac_uk/Et1CEJKr6lNDixyBZv6ApQcB32CNUmTerHOikgHynM9_-Q?e=pjRdLX)

<!-- ```{r setup, include=FALSE} -->
<!-- knitr::opts_chunk$set(echo = TRUE) -->

<!-- ``` -->

## 1. Data import and cleaning

Load data and prepare for analysis:

 - Trim white space on import <br />
 - Rename columns <br />
 - Reformat date column (convert from DD/MM/YY to  YY/MM/DD) <br />
 - Remove empty records <br />
 - Remove duplicate records <br />
 - Sort by date (descending) <br />
 - Add unique ID column
 - Correct ampersand import format
 
```{r echo=TRUE, eval=TRUE}
#> 1.1 Load libraries 
library(tidyverse) # Data muncging and analysis
library(here) # relative path management for reproducibility
library(lubridate) # for date string manipulation/conversion
library(janitor) # data cleaning functions
library(knitr) # report rendering with rmarkdown
library(ggplot2) # plots and visualisations

#> OLD DB  
#> 1.2 Import data (trimming white space in the process)
db <- read_csv(here("In", "C19_Food_DB_220114.csv"), trim_ws = TRUE)


#> NEW DB  
#> 1.2 Import data (trimming white space in the process)
db <- read_csv(here("In", "C19_Food_DB_220412.csv"), trim_ws = TRUE)


```

Table 1.1 Original database ("db") on import - **`r toString(nrow(db))`** records
```{r echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
#> Glimpse
glimpse(db)
```

```{r echo=TRUE, eval=TRUE}
#> 1.3 Prep for analysis (clean, tidy, transform)
#> Rename columns
db <- db |> 
  rename(AUTH_SOURCE = `Author(s) / Source`, TYPE = `Article Type`, THEME_MAIN=`Theme(s)`, THEME_SUB = `Sub-Theme(s)`) 
for( i in colnames(db)){
    colnames(db)[which(colnames(db)==i)] = toupper(i)
}
#> Remove i
rm(i)
#> Convert date format to YYYY/MM/DD for analysis and sorting by date 
db$DATE <- as.Date(lubridate::parse_date_time(db$DATE, c('dmy', 'ymd')))

#> Remove empty records where all columns are "NA"
# Following filters rows with at least one column not "NA"
db <- janitor::remove_empty(db, which = "rows")


db <- db[complete.cases(db), ]

#> Remove duplicates
db <- db |> distinct()

# Add unique ID column
# First sort by date (oldest first)
db <- db |> 
  arrange(DATE)
# Add ID col
db$UID <- seq.int(nrow(db))
# Bring new UID column to front
db <- db |> select(UID, everything())

#> replace ampersand string "&amp;" with "&" in selected columns
db$THEME_MAIN <- (gsub("&amp;", "&", db$THEME_MAIN))
db$THEME_SUB <- (gsub("&amp;", "&", db$THEME_SUB))

#> Export as CSV
write_csv(db, here("Out", "Descriptive_Stats", "1_Current_Database_Cleaned", paste0("C19_Food_DB_Cleaned_", Sys.Date(), ".csv")))

```
Table 1.2 Processed database after cleaning and pre-processing**`r toString(nrow(db))`** records
```{r echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
#> Glimpse
glimpse(db)
```
*(**Note 20/01/21** - requires further processing - e.g. new cols for shortened theme names)*
<br /> <br />



## 2. Themes: Count

This section includes tables and plots showing the summary counts of main themes and sub-themes in the database. 

### 2.1 Main Themes
This section shows the summary counts for main themes. <br /> <br />
**Note:** a small number of database entries have been allocated multiple main themes (see bottom of table). **Damian** - these can be left as is, or split and counted as single themes - let me know. <br /> <br />
**Also** - note that very long theme names affect the format of tables and plots - we need an alternative set of concise theme names for both main and sub-themes to improve presentation (task for DM and SR).

```{r echo=TRUE, eval=TRUE, fig.height = 6, fig.width = 9}


#> Firstly, count and plot number of articles by month
t2.1.articles <- db |> 
  group_by(month = lubridate::floor_date(DATE, "month")) |> 
  summarise(count = n())
#> Export table as CSV file
write_csv(t2.1.articles , here("Out", "Descriptive_Stats", "2_Themes_Count", "2.1_Main_Themes", "Table_2.1.1_Article count by month.csv"))
#> Plot as bar chart
p2.1 <- ggplot(t2.1.articles, aes(x = month, y = count)) +
  theme_bw() +
  # ggtitle(paste0("Figure 2.2.", i , ": ", theme.m)) +
  geom_bar(stat = "identity", fill = 	"#599ad3") +
  ggtitle(paste0("Article count by month")) +
  theme(axis.title.y = element_blank()) +
  theme(legend.position="none") +
  scale_x_date(NULL, date_labels = "%b %y", date_breaks = "2 months") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
p2.1
ggsave(here("Out", "Descriptive_Stats", "2_Themes_Count", "2.1_Main_Themes", "Plot2.1.1_Article_Count_By_Month.png"), width = 9, height = 6)
# #> Drop day from month string for web rendering
# t2.1.articles$month <- format(t2.1.articles$month, "%Y/%m")
#> Render table on web page
knitr::kable(t2.1.articles, col.names = c("Month", "Count"), caption = "Table 2.1.1 Article count by month")




#> Group and count main themes
t2.2.theme.m.sum <- db |> 
  select(THEME_MAIN) |> 
  group_by(THEME_MAIN) |> 
  summarise(Count = n()) |> 
  arrange(desc(Count))
#> Render table on web page
knitr::kable(t2.2.theme.m.sum, col.names = c("Main theme", "Article count"), caption = "Table 2.1.2 Article count by main theme")
#> Export table as CSV file
write_csv(t2.2.theme.m.sum, here("Out", "Descriptive_Stats", "2_Themes_Count", "2.1_Main_Themes", "Table2.1.2_THEMES_MAIN_Count.csv"))
#> Create (and export) bar plot
p2.2 <- ggplot(t2.2.theme.m.sum, aes(x = reorder(THEME_MAIN, Count), y = Count)) +
  theme_bw() +
  ggtitle("Figure 2.1.2 Article count by main theme") +
  geom_bar(stat = "identity", fill = "#9e66ab") +
  theme(axis.title.y = element_blank()) +
  coord_flip() 
p2.2
ggsave(here("Out", "Descriptive_Stats", "2_Themes_Count", "2.1_Main_Themes", "Plot2.1.2_THEMES_MAIN_Count.png"), width = 9, height = 6)
  

```


### 2.2 Sub-Themes

This section shows the summary counts for sub-themes.

```{r echo=TRUE, eval=TRUE, results='asis', fig.height = 6, fig.width = 9}


#> Create list of main themes for iteration
t2.2_theme.m <- t2.2.theme.m.sum |> 
  slice(1:19) 
themes <- as.list(t2.2_theme.m$THEME_MAIN)
#> Sequential integer counter for figure numbers
i <-0 
#> Initiate loop
for(theme.m in themes){
#> Loop counter (for table caption number)
i <- i+1

#> Create data table (count of articles by sub-theme per main theme)
t2.2_loop <- db |>
 select(THEME_MAIN, THEME_SUB) |>
 filter(THEME_MAIN == theme.m) |>
 group_by(THEME_MAIN, THEME_SUB) |>
 summarise(Count = n()) |> 
 ungroup() |> 
 select(THEME_SUB, Count) |>
 arrange(desc(Count)) 
#> Remove main theme from THEME_SUB string (i.e. all text before and included hyphen)
t2.2_loop$THEME_SUB <- gsub("^.*?\\-","", t2.2_loop$THEME_SUB)
#> Trim WS
t2.2_loop$THEME_SUB <- trimws(t2.2_loop$THEME_SUB)

print(knitr::kable(t2.2_loop, col.names = c("Sub-theme", "Article count"), caption = paste0("Table 2.2.", i , ": ", theme.m)))
#> Add line break
cat("\n")
cat("\n")
cat("\n")


#> Export table as CSV file
write_csv(t2.2_loop, here("Out", "Descriptive_Stats", "2_Themes_Count", "2.2_Sub_Themes", paste0("Table 2.2.", i, "_", "_SUB_THEMES_Count.csv")))
#> Create (and export) bar plot
p2.2 <- ggplot(t2.2_loop, aes(x = reorder(THEME_SUB, Count), y = Count)) +
  theme_bw() +
  ggtitle(paste0("Figure 2.2.", i , ": ", theme.m)) +
  geom_bar(stat = "identity", fill = 	"#599ad3") +
  theme(axis.title.y = element_blank()) +
  theme(legend.position="none") +
  coord_flip()

#> Render on web page
print(p2.2)
#> Export plots
ggsave(here("Out", "Descriptive_Stats", "2_Themes_Count", "2.2_Sub_Themes", paste0("Figure_2.2.", i, "_", ".png")), width = 9, height = 6)

#> End loop
}

```

```{r}

#> Remove unwanted variables
rm(i)
```
<br />
<br />




## 3. Themes: Temporal analysis

In this section article occurrences are plotted over time by main theme. The pink rectanges in each plot indicate the time periods of the main Covid lockdowns in England (source: [**HERE**](https://www.instituteforgovernment.org.uk/charts/uk-government-coronavirus-lockdowns))



### 3.1 Main Themes

```{r echo=TRUE, eval=TRUE, results='asis', fig.height = 6, fig.width = 9}

#> Data frame to hold date ranges for lockdows
#> See: https://stackoverflow.com/questions/61479286/how-to-annotate-time-periods-on-line-plots-with-date-axis
Lockdowns <- data.frame(date_start= as_date(c("2020-03-23", "2020-11-05", "2021-01-06")),
                        date_end = as_date(c("2020-05-10","2020-12-02", "2021-03-08")))

#> Sequential integer counter for figure numbers
i <- 0 
#> Initiate loop
for(theme.m in themes){
#> Loop counter (for table caption number)
i <- i+ 1

#> Non loop version
#> Get a data frame for one main theme just with ID and dates
#> Aggregate by week
#< Tip from: https://stackoverflow.com/questions/47503159/ggplot-using-grouped-date-variables-such-as-year-month
t3.1_theme.m.t <- db |> 
  select(UID, THEME_MAIN, DATE) |>  
  filter(THEME_MAIN == theme.m) |> 
  mutate(OBS = 1) |> 
  mutate(Month = floor_date(DATE, unit = "month")) |> 
  group_by(Month) |> 
  summarise(Count = sum(OBS))
#> Get maximum value of Count for use in plotting
max_val <- max(t3.1_theme.m.t$Count)

#> Plot
p3.1 <- ggplot()+
  theme_bw() +
  ggtitle(paste0("Figure 3.1.", i , ": ", theme.m)) +
  geom_rect(data = Lockdowns, aes(xmin = date_start, xmax = date_end, ymin = -Inf, ymax = Inf),
            fill = "red", alpha= 0.3) +
  geom_line(data = t3.1_theme.m.t, aes(x=Month, y=Count), size = 1) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_x_date(NULL, date_labels = "%b %y", date_breaks = "2 months") +
  scale_y_continuous(breaks = seq(0, max_val, by = 2)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
p3.1

#> Render on web page
print(p3.1)

#> Export plots
ggsave(here("Out", "Descriptive_Stats", "3_Themes_Temporal", "3.1_Main_Themes_Plots", paste0("Figure_3.1.", i, "_", ".png")), width = 9, height = 6)

#> Add line break
cat("\n")
cat("\n")
cat("\n")
# #> Export plots
# ggsave(here("Out", "Descriptive_Stats", "2_Themes", "2.2_Sub_Themes", paste0("Table 2.2.", i, ".png")), width = 9, height = 6)

#> End loop
}

```



## 4. Keywords: Count

Summary count of keywords in the database. 

```{r echo=TRUE, eval=TRUE,results='asis', fig.height = 6, fig.width = 9}
#> Split keyword string and create long format table - i.e. 1 keyword per row
t4.1.kwords <- db |> 
  mutate(Keyword = strsplit(as.character(KEYWORDS), ",")) |> 
  unnest(Keyword)
#> Trim white space
t4.1.kwords$Keyword <- trimws(t4.1.kwords$Keyword)
#> Remove duplicates and recount
t4.1.kwords.count <- t4.1.kwords |> 
  group_by(Keyword) |> 
  summarise(Count = n()) |> 
  arrange(desc(Count))
#> Render table on web page
knitr::kable(t4.1.kwords.count, col.names = c("Keyword", "Count"), caption = "Table 4.1 Keyword count")
#> Export table as CSV file
write_csv(t4.1.kwords.count, here("Out", "Descriptive_Stats", "4_Keywords_Count", "Tables", "Table4.1_Keywords_Count.csv"))


```

## 5. Keywords: Temporal analysis

Analysis of keyword frequency plotted over time, with periods of lockdown shown. The top 20 most frequently occurring keywords are shown (this can be easily extended to more or all keywords)

```{r echo=TRUE, eval=TRUE,results='asis', fig.height = 6, fig.width = 9}

#> Get the 20 most frequently ocurring keywords
t5.1.kwords.top20 <- t4.1.kwords.count |> 
slice_max(Count, n = 20) |> 
# drop unwanted Count column
select(Keyword)

#> Merge with main keywords table to extract top 20
t5.1.kwords.plot <- merge(t4.1.kwords, t5.1.kwords.top20, by = "Keyword")

#> Get list of top 20 keywords for loop
kwords <- as.list(t5.1.kwords.top20$Keyword)

#> Sequential integer counter for figure numbers
i <-0 
#> Initiate loop
for(kword in kwords){
#> Loop counter (for table caption number)
i <- i+1

#> Create data table (count of articles by sub-theme per main theme)
t5.1_loop <- t4.1.kwords |>
 select(Keyword, DATE) |>
 filter(Keyword == kword) |> 
 mutate(OBS = 1) |> 
 mutate(Month = floor_date(DATE, unit = "month")) |> 
 group_by(Month) |> 
 summarise(Count = sum(OBS))

#> Get maximum value of Count for use in plotting
max_val <- max(t5.1_loop$Count)

#> Plot
p5.1 <- ggplot()+
  theme_bw() +
  ggtitle(paste0("Figure 5.", i , ": ", kword)) +
  geom_rect(data = Lockdowns, aes(xmin = date_start, xmax = date_end, ymin = -Inf, ymax = Inf),
            fill = "red", alpha= 0.3) +
  geom_line(data = t5.1_loop, aes(x=Month, y=Count), size = 1) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_x_date(NULL, date_labels = "%b %y", date_breaks = "2 months") +
  scale_y_continuous(breaks = seq(0, max_val, by = 2)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
p5.1

#> Render on web page
print(p5.1)

#> Export plots
ggsave(here("Out", "Descriptive_Stats", "5_Keywords_Temporal", "Plots", paste0("Figure_5.", i, "_", kword, ".png")), width = 9, height = 6)


}

```


```{r}
#> Remove unwanted variables
rm(i)

```


## 6. Articles by type


```{r echo=TRUE, eval=TRUE, fig.height = 6, fig.width = 9}
#> Group and count article types
t6.1.article.sum <- db |> 
  select(TYPE) |> 
  group_by(TYPE) |> 
  summarise(Count = n()) |> 
  arrange(desc(Count))
#> Render table on web page
knitr::kable(t6.1.article.sum, col.names = c("Article Type", "Count"), caption = "Table 6.1 Article type (summary count)")
#> Export table as CSV file
write_csv(t6.1.article.sum, here("Out", "Descriptive_Stats", "6_Article_Type", "Table", "Table6.1_Article_Type_Count.csv"))
#> Create (and export) bar plot
p6.1 <- ggplot(t6.1.article.sum, aes(x = reorder(TYPE, Count), y = Count)) +
  theme_bw() +
  ggtitle("Figure 6.1 Article type") +
  geom_bar(stat = "identity", fill = "#d77fb3") +
  theme(axis.title.y = element_blank()) +
  coord_flip() 
p6.1
ggsave(here("Out", "Descriptive_Stats", "6_Article_Type", "Plot", "Plot6.1_Article_Type_Count.png"), width = 9, height = 6)
  

```




## 7. Topic modelling

Topic modelling of "summary" string variable. 

Workflow:

 - Load "stm" package and other ancillary libraries for topic modelling  <br />
 - Load the cleaned and processed Covid Food Database (Step 1)
 - Compute some basic descriptive statistics (e.g. histogram showing number 
 - Pre-process data (Step 7.2)
 - Estimate models (Step 7.3)
 - Evaluate models(Step 7.4)


### 7.1 Libraries and database prep

```{r echo=TRUE, eval=TRUE, fig.height = 6, fig.width = 9}

#> Load libraries for topic modelling
library(stm) # Structural topic modelling
library(tidystm) # devtools::install_github("mikajoh/tidystm", dependencies = TRU

#> Load tidied database (exported in Step 1)
db <- read_csv(here("Out", "Descriptive_Stats", "1_Current_Database_Cleaned", "C19_Food_DB_Cleaned_2022-04-12.csv"), trim_ws = TRUE)


#> Data source for topic modelling is the "SUMMARY" column
#> Extract the UID, SUMMARY, and DATE columns from main table
data <- db |>
  select(UID, SUMMARY, DATE, THEME_MAIN)

#> Count words in each summary
db$SUMMARY.count <- str_count(db$SUMMARY, "\\w+")

#> Summary table of word count in SUMMARY.count columns
summary.counts <- db |> 
  group_by(SUMMARY.count) |> 
  summarise(n = n()) |> 
  arrange(desc(SUMMARY.count))


#> Render table on web page
knitr::kable(summary.counts, col.names = c("No_of_words", "No_of_articles"), caption = "Table 7.1 No of words in article summaries")
#> Export table as CSV file
write_csv(summary.counts, here("Out", "Topic_modelling", "SUMMARY_stats", "Table 7.1 No of words in article summaries.csv"))
#> Create (and export) bar plot
p7.1 <- ggplot(summary.counts, aes(x = SUMMARY.count, y = n)) +
  theme_bw() +
  ggtitle("Figure 7.1 No of words in article summaries") +
  geom_bar(stat = "identity", fill = "#d77fb3") +
  # theme(axis.title.y = element_blank()) +
  xlab("No. of words") +
  ylab("Word count")
p7.1
ggsave(here("Out", "Topic_modelling", "SUMMARY_stats", "Plot7.1_No of words in article summaries.png"), width = 9, height = 6)



```

### 7.2 Pre-processing

Summary of data pre-processing steps:

 - Convert data for analysis in "stm" package using textProcessor function - performs stemming (reducing words to their root form), drops punctuation and removes stop words (e.g., the, is, at, I etc.) <br />
 - Process the loaded data using "prepDocuments" function and remove infrequent terms


```{r echo=TRUE, eval=TRUE, fig.height = 6, fig.width = 9}



#> Here we need to pick out records for only the selected main themes - "Food Access & Security (UK)", AND "Agri-food Labour"
topic.themes <- c("Food Access & Security (UK)", "Agri-food Labour")

#> ONCE TOPICS CODE HAS BEEN TESTED THEN USE LOOP TO RUN TOPICS ANALYSIS ON BOTH THEMES - CREATING TWO MAIN SEPARATE FOLDERS FOR EACH TOPIC


#> Filter main database, extracting only records related to active topic
data <- data |>
  filter(THEME_MAIN == "Food Access & Security (UK)")

#> textProcessor
#> Read in and pre-process data (i.e. stemming/stopword removal, etc.) using textProcessor function
processed <- textProcessor(data$SUMMARY, metadata=data)
?textProcessor
#> Print a documents list object (e.g. "document 1")
# print(processed[["documents"]][["1"]])


#> prepDocuments
#> Evaluate how many words and documents would be removed from the data set at each word threshold, which is the minimum number of documents a word needs to appear in order for the word to be kept within the vocabulary. 
fig7.2.1 <- png(here("Out", "Topic_modelling", "7.2_Pre-processing", "Lower_Word_Threshold_Evaluation.png"), width = 800, height = 600)
plotRemoved(processed$documents, lower.thresh = seq(1, 200, by = 100))
fig7.2.1
dev.off()




#> Then select the preferred threshold within prepDocuments. 
#> Importantly, prepDocuments will also re-index all metadata/document relationships if any changes occur due to processing.
#> Run prepDocuments - processes the loaded data to make sure it is in the right format and remove infrequent terms depending on the user-set parameter "lower.thresh"
out <- prepDocuments(processed$documents, processed$vocab, processed$meta,  lower.thresh = 10, upper.thresh = Inf, subsample = NULL, verbose = TRUE)
?prepDocuments

#> Main objects created for use with stm:
#> 1. The new documents object for use with stm
docs <- out$documents
#> 2. The new vocab object for use with stm
vocab <- out$vocab
#> 3. The new meta data object for use with stm. Will be the same if no documents are removed.
meta <-out$meta

#> Other object generated by prepDocuments
#> 4. A set of indices corresponding to the positions in the original vocab object of words which have been removed
w.rem <- out$words.removed
print(w.rem)
#> 5. A set of indices corresponding to the positions in the original documents object of documents which no longer contained any words after dropping terms from the vocab
d.rem <- out$docs.removed
print(d.rem)
#> 6. An integer corresponding to the number of unique tokens removed from the corpus
t.rem <- out$tokens.removed
print(t.rem)
#> 7. A table giving the the number of documents that each word is found in of the original document set, prior to any removal. This can be passed through a histogram for visual inspection
w.count <- out$wordcounts
print(w.count)

```



### 7.3 Estimate (run) and model(s)

 - Run structural topic model with different numbers of topics (K) (3, 4, 5, 6, 9, 12, 15, 18, and 21) <br />
 - Run stm with K=0 to assess usefulness for determining possible range for "true" K


```{r echo=TRUE, message=FALSE, results='hide', eval=TRUE, fig.height = 6, fig.width = 9}


#> 4.2  EVALUATING THE STRUCTURAL TOPIC MODEL
#> Estimation with topical prevalence parameter (Year of publication) - as a covariate

#> Allocate plenty of memory for stm runs
memory.size()
memory.limit(size=500000)

#> 4.2.1 - Estimate choice of number of topics using searchK function
set.seed(54321)
K <-c (3, 6, 9, 12, 15, 18, 21)
k.result <- stm::searchK(out$documents, out$vocab, K, prevalence =~ DATE, data = out$meta)


#> Plot the diganostic values of searchK function (e.g. semantic coherence
png(here("Out", "Topic_modelling", "SearchK", "Plot1_searchK_results.png"), width = 800, height = 600)
kplot <- plot.searchK(k.result)
dev.off()
#> Notes:
# - Held-out likelihood essential a measure of complexity - an "old" metric that doesn't provide good "human" results: https://www.youtube.com/watch?v=rfHCronRgQU&t=42s
#> Note, the results of searchK may not be as useful as comparing semantic coherence with exclusivity (see later)



#> FITTING OF MODEL ANMD EVALUATION OF #K (topics)
#> Generate a range of stm models with different values of K
#> For each model, generate a series of outputs, including top topics, semantic coherence vs. exclusivity, and word clouds - in order to choose "final K"
#> Chosen 10 to 50 in 10 increments of 10 as we think more than 50 topics would be difficult to interpret given time and resources

library(Rtsne)
library(rsvd)
library(geometry)

#> K = 0
#> When initialization type is set to "Spectral" the user can specify K = 0 to use the algorithm of Lee and Mimno (2014) to select the number of topics. The core idea of the spectral initialization is to approximately find the vertices of the convex hull of the word co-occurrences. The algorithm of Lee and Mimno (2014) projects the matrix into a low dimensional space using t-distributed stochastic neighbor embedding (Van der Maaten 2014) and then exactly solves for the convex hull. This has the advantage of automatically selecting the number of topics. The added randomness from the projection means that the algorithm is not deterministic like the standard "Spectral" initialization type. Running it with a different seed can result in not only different results but a different number of topics. We emphasize that this procedure has no particular statistical guarantees and should not be seen as estimating the “true” number of topics. However it can be useful to start and has the computational advantage that it only needs to be run once.
stm.0 <- stm::stm(documents = out$documents, vocab = out$vocab, K = 0, prevalence =~ DATE, max.em.its = 1000, data = out$meta, init.type = "Spectral")


#> Run stm with specified number of topics (note: no need to set seed with spectral initialization)
# 
# #> K = 2 - DOES NOT WORK WITH ONLY 2 TOPICS
# stm.2 <- stm::stm(documents = out$documents, vocab = out$vocab, K = 2, prevalence =~ DATE, max.em.its = 100, data = out$meta, init.type = "Spectral")
#> K = 3 
stm.3 <- stm::stm(documents = out$documents, vocab = out$vocab, K = 3, prevalence =~ DATE, max.em.its = 100, data = out$meta, init.type = "Spectral")
#> K = 4 
stm.4 <- stm::stm(documents = out$documents, vocab = out$vocab, K = 4, prevalence =~ DATE, max.em.its = 100, data = out$meta, init.type = "Spectral")
#> K = 5
stm.5 <- stm::stm(documents = out$documents, vocab = out$vocab, K = 5, prevalence =~ DATE, max.em.its = 100, data = out$meta, init.type = "Spectral")
#> K = 6
stm.6 <- stm::stm(documents = out$documents, vocab = out$vocab, K = 6, prevalence =~ DATE, max.em.its = 100, data = out$meta, init.type = "Spectral")
#> K = 9
stm.9 <- stm::stm(documents = out$documents, vocab = out$vocab, K = 9, prevalence =~ DATE, max.em.its = 100, data = out$meta, init.type = "Spectral")
#> K = 12
stm.12 <- stm::stm(documents = out$documents, vocab = out$vocab, K = 12, prevalence =~ DATE, max.em.its = 100, data = out$meta, init.type = "Spectral")
#> K = 15
stm.15 <- stm::stm(documents = out$documents, vocab = out$vocab, K = 15, prevalence =~ DATE, max.em.its = 100, data = out$meta, init.type = "Spectral")
#> K = 18
stm.18 <- stm::stm(documents = out$documents, vocab = out$vocab, K = 18, prevalence =~ DATE, max.em.its = 100, data = out$meta, init.type = "Spectral")
#> K = 21
stm.21 <- stm::stm(documents = out$documents, vocab = out$vocab, K = 21, prevalence =~ DATE, max.em.its = 100, data = out$meta, init.type = "Spectral")


# with spline "s()" applied to DATE

# #> K = 3 
# stm.2.sp <- stm::stm(documents = out$documents, vocab = out$vocab, K = 2, prevalence =~ s(DATE), max.em.its = 100, data = out$meta, init.type = "Spectral")
# #> K = 3 
# stm.3.sp <- stm::stm(documents = out$documents, vocab = out$vocab, K = 3, prevalence =~ s(DATE), max.em.its = 100, data = out$meta, init.type = "Spectral")
# #> K = 4 
# stm.4.sp <- stm::stm(documents = out$documents, vocab = out$vocab, K = 4, prevalence =~ s(DATE), max.em.its = 100, data = out$meta, init.type = "Spectral")
# #> K = 4 
# stm.5.sp <- stm::stm(documents = out$documents, vocab = out$vocab, K = 5, prevalence =~ s(DATE), max.em.its = 100, data = out$meta, init.type = "Spectral")
# #> K = 6
# stm.6.sp <- stm::stm(documents = out$documents, vocab = out$vocab, K = 6, prevalence =~ s(DATE), max.em.its = 100, data = out$meta, init.type = "Spectral")
# #> K = 9
# stm.9.sp <- stm::stm(documents = out$documents, vocab = out$vocab, K = 9, prevalence =~ s(DATE), max.em.its = 100, data = out$meta, init.type = "Spectral")
# #> K = 12
# stm.12.sp <- stm::stm(documents = out$documents, vocab = out$vocab, K = 12, prevalence =~s(DATE), max.em.its = 100, data = out$meta, init.type = "Spectral")
# #> K = 15
# stm.15.sp <- stm::stm(documents = out$documents, vocab = out$vocab, K = 15, prevalence =~ s(DATE), max.em.its = 100, data = out$meta, init.type = "Spectral")
# #> K = 18
# stm.18.sp <- stm::stm(documents = out$documents, vocab = out$vocab, K = 18, prevalence =~ s(DATE), max.em.its = 100, data = out$meta, init.type = "Spectral")
# #> K = 21
# stm.21.sp <- stm::stm(documents = out$documents, vocab = out$vocab, K = 21, prevalence =~ s(DATE), max.em.its = 100, data = out$meta, init.type = "Spectral")


```

### 7.4 Evaluate models

Evaluate range of models produced in  Section 7.2 to determine K <br />

 - Plot semantic coherence against exclusivity for all plots <br />
 - Interpret topics for each stm run using a combination of interactive LDAviz plots, word clouds <br />

<br />
<br />
<br />


<br />
<br />
#### 7.4.1 Semantic coherence vs. Exclusivity



```{r echo=FALSE, message=FALSE, results='hide', eval=TRUE, fig.height = 6, fig.width = 9}


#> For each STM model plot semantic coherence against exclusivity - see: https://francescocaberlin.blog/2019/06/26/messing-around-with-stm-part-iiia-model-selection/
#> First generate a data frame for each stm with exclusivity and semantic coherence variables for each K topic

#> THIS NEEDS TO BE AUTOMATED IN LOOP

#> stm.3
sem.excl.3 <- as.data.frame(cbind(c(1:3), exclusivity(stm.3), semanticCoherence(model=stm.3, docs), "Mod_3"))
colnames(sem.excl.3) <- c("K", "Exclusivity", "SemanticCoherence", "Model")

#> stm.4
sem.excl.4 <- as.data.frame(cbind(c(1:3), exclusivity(stm.4), semanticCoherence(model=stm.3, docs), "Mod_4"))
colnames(sem.excl.4) <- c("K", "Exclusivity", "SemanticCoherence", "Model")

#> stm.5
sem.excl.5 <- as.data.frame(cbind(c(1:3), exclusivity(stm.5), semanticCoherence(model=stm.5, docs), "Mod_5"))
colnames(sem.excl.5) <- c("K", "Exclusivity", "SemanticCoherence", "Model")

#> stm.6
sem.excl.6 <- as.data.frame(cbind(c(1:6), exclusivity(stm.6), semanticCoherence(model=stm.6, docs), "Mod_6"))
colnames(sem.excl.6) <- c("K", "Exclusivity", "SemanticCoherence", "Model")

#> stm.9
sem.excl.9 <- as.data.frame(cbind(c(1:9), exclusivity(stm.9), semanticCoherence(model=stm.9, docs), "Mod_9"))
colnames(sem.excl.9) <- c("K", "Exclusivity", "SemanticCoherence", "Model")

#> stm.12
sem.excl.12 <- as.data.frame(cbind(c(1:12), exclusivity(stm.12), semanticCoherence(model=stm.12, docs), "Mod_12"))
colnames(sem.excl.12) <- c("K", "Exclusivity", "SemanticCoherence", "Model")

#> stm.15
sem.excl.15 <- as.data.frame(cbind(c(1:15), exclusivity(stm.15), semanticCoherence(model=stm.15, docs), "Mod_15"))
colnames(sem.excl.15) <- c("K", "Exclusivity", "SemanticCoherence", "Model")

#> stm.18
sem.excl.18 <- as.data.frame(cbind(c(1:18), exclusivity(stm.18), semanticCoherence(model=stm.18, docs), "Mod_18"))
colnames(sem.excl.18) <- c("K", "Exclusivity", "SemanticCoherence", "Model")

#> stm.21
sem.excl.21 <- as.data.frame(cbind(c(1:21), exclusivity(stm.21), semanticCoherence(model=stm.21, docs), "Mod_21"))
colnames(sem.excl.21) <- c("K", "Exclusivity", "SemanticCoherence", "Model")



#> Bind the data frames together for plotting

#> 1. All Models
ModsExSem.all <- rbind(sem.excl.3, sem.excl.4, sem.excl.5, sem.excl.6, sem.excl.9, sem.excl.12, sem.excl.15, sem.excl.18, sem.excl.21)
#> Change values in data frame to numeric
ModsExSem.all$Exclusivity <- as.numeric(as.character(ModsExSem.all$Exclusivity))
ModsExSem.all$SemanticCoherence <- as.numeric(as.character(ModsExSem.all$SemanticCoherence))

#> 2. Low number models (3,4,5.6)
ModsExSem.Low <- rbind(sem.excl.3, sem.excl.4, sem.excl.5, sem.excl.6)
#> Change values in data frame to numeric
ModsExSem.Low$Exclusivity <- as.numeric(ModsExSem.Low$Exclusivity)
ModsExSem.Low$SemanticCoherence <- as.numeric(ModsExSem.Low$SemanticCoherence)



#> STM SEMANTIC COHERENCE - EXCLUSIVITY PLOTS
#> Run the semantic coherence vs. exclusivity plot for all 6 stm models

#> 1. All models
plot.ModsExSem.all <-ggplot(ModsExSem.all, aes(SemanticCoherence, Exclusivity, color = Model))+geom_point(size = 2, alpha = 0.7) + 
  geom_text(aes(label=Model), nudge_x=.03, nudge_y=.03, size = 1.5) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Exclusivity vs. semantic coherence: All Models")
plot.ModsExSem.all
#> Export plot
ggsave(here("Out", "Topic_modelling", "SemanticCoherence_Exclusivity", "Plot1a_Semantic_Exclusivity_allSTM_labels.png"), width = 9, height = 7, dpi = 300)

#> 2. All models
plot.ModsExSem.Low <-ggplot(ModsExSem.Low, aes(SemanticCoherence, Exclusivity, color = Model))+geom_point(size = 5, alpha = 0.7) + 
  geom_text(aes(label=Model), nudge_x=.03, nudge_y=.03, size = 2) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence")
plot.ModsExSem.Low
#> Export plot
ggsave(here("Out", "Topic_modelling", "SemanticCoherence_Exclusivity", "Plot2a_Semantic_Exclusivity_LowSTM_labels.png"), width = 9, height = 7, dpi = 300)



#> Can also try the "rPref" package to compute a Pareto-optimal set of models maximizing both metrics
#> See https://p-roocks.de/rpref/


```


#### 7.4.2 STM plots


Plots of the different STM models showing topic proportions and the top 3 words associated with each topic <br />


```{r echo=TRUE, eval=TRUE, fig.height = 6, fig.width = 9}


plot(stm.3, n=3, main = "STM 3")

plot(stm.4, n=4, main = "STM 4")

plot(stm.5, n=5, main = "STM 5")

plot(stm.6, n=6, main = "STM 6")

plot(stm.9, n=9, main = "STM 9")

plot(stm.12, n=12, main = "STM 12")

plot(stm.15, n=15, main = "STM 15")

plot(stm.18, n=18, main = "STM 18")

plot(stm.21, n=21, main = "STM 21")


```






#### 7.4.3 'LDAvis' interactive plots


The LDAvis R package is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization. 

LDAvis outputs have been generated for each of the model runs. Click on the links below to explore the interactive model visualisations:

**[3 topic model](https://robertberryuk.github.io/c19food/stm_3/index.html)<br />**
**[4 topic model](https://robertberryuk.github.io/c19food/stm_4/index.html)<br />**
**[5 topic model](https://robertberryuk.github.io/c19food/stm_5/index.html)<br />**
**[6 topic model](https://robertberryuk.github.io/c19food/stm_6/index.html)<br />**
**[9 topic model](https://robertberryuk.github.io/c19food/stm_9/index.html)<br />**
**[12 topic model](https://robertberryuk.github.io/c19food/stm_12/index.html)<br />**
**[15 topic model](https://robertberryuk.github.io/c19food/stm_15/index.html)<br />**
**[18 topic model](https://robertberryuk.github.io/c19food/stm_18/index.html)<br />**
**[21 topic model](https://robertberryuk.github.io/c19food/stm_21/index.html)<br />**
<br/><br/>
<br/><br/>



```{r echo=FALSE, message=FALSE, results='hide', eval=TRUE, fig.height = 6, fig.width = 9}


#> LDAvis - create interactive hmtl visualisations of the stm models and their topics


#> Import LDAvis library
library(LDAvis)

#> Interpreting LDA - https://stats.stackexchange.com/questions/448091/what-does-each-graph-quadrant-mean-with-ldavis-plot-in-r

#> Delete any existing LDAvis outputs in project folder
unlink(here("Out", "Topic_modelling", "LDAvis", "/*" ))

#> Model stm.3
stm::toLDAvis(
  stm.3,
  docs,
  R = 30,
  plot.opts = list(xlab = "PC1", ylab = "PC2"),
  lambda.step = 0.01,
  out.dir = here("Out", "Topic_modelling", "LDAvis", "stm_3"),
  open.browser = interactive(),
  as.gist = FALSE,
  reorder.topics = TRUE
)


#> Model stm.4
stm::toLDAvis(
  stm.4,
  docs,
  R = 30,
  plot.opts = list(xlab = "PC1", ylab = "PC2"),
  lambda.step = 0.01,
  out.dir = here("Out", "Topic_modelling", "LDAvis", "stm_4"),
  open.browser = interactive(),
  as.gist = FALSE,
  reorder.topics = TRUE
)


#> Model stm.5
stm::toLDAvis(
  stm.5,
  docs,
  R = 30,
  plot.opts = list(xlab = "PC1", ylab = "PC2"),
  lambda.step = 0.01,
  out.dir = here("Out", "Topic_modelling", "LDAvis", "stm_5"),
  open.browser = interactive(),
  as.gist = FALSE,
  reorder.topics = TRUE
)



#> Model stm.6
stm::toLDAvis(
  stm.6,
  docs,
  R = 30,
  plot.opts = list(xlab = "PC1", ylab = "PC2"),
  lambda.step = 0.01,
  out.dir = here("Out", "Topic_modelling", "LDAvis", "stm_6"),
  open.browser = interactive(),
  as.gist = FALSE,
  reorder.topics = TRUE
)


#> Model stm.9
stm::toLDAvis(
  stm.9,
  docs,
  R = 30,
  plot.opts = list(xlab = "PC1", ylab = "PC2"),
  lambda.step = 0.01,
  out.dir = here("Out", "Topic_modelling", "LDAvis", "stm_9"),
  open.browser = interactive(),
  as.gist = FALSE,
  reorder.topics = TRUE
)


#> Model stm.12
stm::toLDAvis(
  stm.12,
  docs,
  R = 30,
  plot.opts = list(xlab = "PC1", ylab = "PC2"),
  lambda.step = 0.01,
  out.dir = here("Out", "Topic_modelling", "LDAvis", "stm_12"),
  open.browser = interactive(),
  as.gist = FALSE,
  reorder.topics = TRUE
)


#> Model stm.15
stm::toLDAvis(
  stm.15,
  docs,
  R = 30,
  plot.opts = list(xlab = "PC1", ylab = "PC2"),
  lambda.step = 0.01,
  out.dir = here("Out", "Topic_modelling", "LDAvis", "stm_15"),
  open.browser = interactive(),
  as.gist = FALSE,
  reorder.topics = TRUE
)


#> Model stm.18
stm::toLDAvis(
  stm.18,
  docs,
  R = 30,
  plot.opts = list(xlab = "PC1", ylab = "PC2"),
  lambda.step = 0.01,
  out.dir = here("Out", "Topic_modelling", "LDAvis", "stm_18"),
  open.browser = interactive(),
  as.gist = FALSE,
  reorder.topics = TRUE
)

#> Model stm.21
stm::toLDAvis(
  stm.21,
  docs,
  R = 30,
  plot.opts = list(xlab = "PC1", ylab = "PC2"),
  lambda.step = 0.01,
  out.dir = here("Out", "Topic_modelling", "LDAvis", "stm_21"),
  open.browser = interactive(),
  as.gist = FALSE,
  reorder.topics = TRUE
)





```



#### 7.4.4 Word clouds

<br />
<br />

#### Other outputs to aid model evaluation (e.g. findThoughts, 
<br />
<br />

## 8. Topics: Temporal analysis
<br />
<br />
Plotting latent topics over time


<!-- # ```{r echo=FALSE, message=FALSE, results='hide, eval=TRUE, fig.height = 6, fig.width = 9} -->
<!-- #  -->
<!-- #  -->
<!-- # #> Topic summaries - https://stackoverflow.com/questions/69097388/plot-more-than-3-words-per-topic-for-stm -->
<!-- #  -->
<!-- # #> Word clouds -->
<!-- #  -->
<!-- # #> MAY HAVE TO USE LOOP? -->
<!-- # topic.list <- 1:25 -->
<!-- # cloud(stm.25, topic = 2) -->
<!-- # cloud(stm.25, topic = 14) -->
<!-- # cloud(stm.25, topic = 20) -->
<!-- # cloud(stm.25, topic = 1) -->
<!-- # cloud(stm.25, topic = 3) -->
<!-- # cloud(stm.25, topic = 4) -->
<!-- # cloud(stm.25, topic = 5) -->
<!-- # cloud(stm.25, topic = 6) -->
<!-- #  -->
<!-- # plot(stm.18) -->
<!-- #  -->
<!-- #  -->
<!-- #  -->

<!-- #  -->
<!-- #  -->
<!-- #  -->
<!-- # #> Generate a set of words describing each topic from each fitted STM model  -->
<!-- # topic.words <- 5 -->
<!-- # stm.3.labs <- stm::labelTopics(stm.3, topic.words) -->
<!-- #  -->

<!-- plot(stm.3, n=5, type = "hist") -->




```

